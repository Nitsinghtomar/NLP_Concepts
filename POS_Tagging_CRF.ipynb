{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCMuD_mMKseI",
        "outputId": "f5ec0271-fb67-4eb9-e867-80ad7c7af95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.909\n",
            "Recall: 0.886\n",
            "F1-Score: 0.894\n",
            "\n",
            "Model trained. You can now input sentences for POS tagging.\n",
            "\n",
            "Enter a sentence for POS tagging (or type 'exit' to quit): I am Nitesh Singh\n",
            "\n",
            "POS Tags:\n",
            "i: PRON\n",
            "am: VERB\n",
            "nitesh: ADJ\n",
            "singh: NOUN\n",
            "\n",
            "Enter a sentence for POS tagging (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn_crfsuite import CRF, metrics\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Download dataset\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "def preprocess_data():\n",
        "    data = []\n",
        "    for sentence in brown.tagged_sents(tagset=\"universal\"):\n",
        "        tokens = [word.lower() for word, tag in sentence]\n",
        "        tags = [tag for word, tag in sentence]\n",
        "        data.append((tokens, tags))\n",
        "    return data\n",
        "\n",
        "def extract_features(sentence, i):\n",
        "    word = sentence[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_first': i == 0,\n",
        "        'is_last': i == len(sentence) - 1,\n",
        "        'is_capitalized': word[0].upper() == word[0],\n",
        "        'is_digit': word.isdigit(),\n",
        "        'prev_word': '' if i == 0 else sentence[i - 1],\n",
        "        'next_word': '' if i == len(sentence) - 1 else sentence[i + 1],\n",
        "    }\n",
        "    return features\n",
        "\n",
        "def prepare_dataset(data):\n",
        "    X, y = [], []\n",
        "    for tokens, tags in data:\n",
        "        X.append([extract_features(tokens, i) for i in range(len(tokens))])\n",
        "        y.append(tags)\n",
        "    return X, y\n",
        "\n",
        "# Step 2: Model Training and k-Fold Cross-Validation\n",
        "def train_crf(X, y, k=5):\n",
        "    kf = KFold(n_splits=k)\n",
        "    precision, recall, f1_scores = [], [], []\n",
        "    model = None\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
        "        y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
        "\n",
        "        model = CRF(algorithm='lbfgs', max_iterations=100, all_possible_transitions=True)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        report = metrics.flat_classification_report(y_test, y_pred, output_dict=True, digits=3)\n",
        "\n",
        "        # Overall metrics\n",
        "        precision.append(report[\"macro avg\"][\"precision\"])\n",
        "        recall.append(report[\"macro avg\"][\"recall\"])\n",
        "        f1_scores.append(report[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "    return model, precision, recall, f1_scores\n",
        "\n",
        "# Step 3: Evaluation and Visualization\n",
        "def generate_confusion_matrix(y_test, y_pred, labels):\n",
        "    # Flatten the lists of true and predicted labels\n",
        "    y_true_flat = [item for sublist in y_test for item in sublist]\n",
        "    y_pred_flat = [item for sublist in y_pred for item in sublist]\n",
        "\n",
        "    cm = confusion_matrix(y_true_flat, y_pred_flat, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Main Execution\n",
        "data = preprocess_data()\n",
        "X, y = prepare_dataset(data)\n",
        "\n",
        "# Perform k-fold cross-validation and train the model\n",
        "model, precision, recall, f1_scores = train_crf(X, y)\n",
        "\n",
        "# Overall Metrics\n",
        "print(f\"Precision: {np.mean(precision):.3f}\")\n",
        "print(f\"Recall: {np.mean(recall):.3f}\")\n",
        "print(f\"F1-Score: {np.mean(f1_scores):.3f}\")\n",
        "\n",
        "# Interactive POS Tagging\n",
        "print(\"\\nModel trained. You can now input sentences for POS tagging.\")\n",
        "while True:\n",
        "    sentence = input(\"\\nEnter a sentence for POS tagging (or type 'exit' to quit): \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "    tokens = sentence.lower().split()\n",
        "    features = [extract_features(tokens, i) for i in range(len(tokens))]\n",
        "    predicted_tags = model.predict_single(features)\n",
        "    print(\"\\nPOS Tags:\")\n",
        "    for token, tag in zip(tokens, predicted_tags):\n",
        "        print(f\"{token}: {tag}\")\n"
      ]
    }
  ]
}